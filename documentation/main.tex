\documentclass[11pt]{report}

\usepackage{amsmath, amssymb}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{microtype}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{verbatim}
\usepackage[style=authoryear]{biblatex}
\usepackage{graphicx}

\onehalfspacing

\addbibresource{references.bib}

\title{Bias Mitigation and Detection for LLMs}
\author{Sidharth Kolladikkal Biju}
\date{\today}

\begin{document}
\maketitle

\tableofcontents

\chapter{Introduction}

\section{Motivation}

LLMs are increasingly being used in place of web browsers 
\parencite{PadillaEtAl2025LLMBehavior}, so it's critical to understand how
LLMs handle misinformation and bias. Since users often see one result
from a prompt rather than multiple results in a traditional web browser, users'
diversity of sources have reduced, and therefore makes it far more critical that
we ensure that LLMs produce neutral and factual information, that doesn't
propagate any harmful biases, or misinformation.
\\

It has already been found that LLMs may tend to misclassify left-leaning articles as
neutral \parencite{LinEtAl2025BiasDetection}, and depending on how LLMs are trained
we can inadvertently put inherent bias into the system e.g. ensuring overall performance
might bias the model towards majority groups, and against minority groups
\parencite{RanjanGuptaSingh2024BiasLLMs}. Even modern models that seem to perform
well in mitigating the results of bias, due to the availability of bias testing benchmarks
have been found to be memorising patterns in these benchmarks. Augmenting the prompt reveals
that when the text doesn't match these patterns the LLMs' biases start showing, especially in
biases that aren't as well studied such as age, socio-economic, and appearance based biases
\parencite{MiandoabEtAl2025BreakingBenchmark}.
\\

It seems that traditional fine-tuning techniques such as few-shot prompting, role prompting
contextual prompting, and system prompting \parencite{Chen2025PromptEngineeringReview}, might
work in specific conditions, but can't be reliably generalised when the input doesn't
match the patterns the LLM was trained on \parencite{MiandoabEtAl2025BreakingBenchmark}.
\\

\section{Approach}

I plan to use vocabulary-based matching \parencite{LinEtAl2025BiasDetection} to
identify and evaluate bias. Utilising this as a reliable quantifier for the bias, I
plan to use reinforcement learning to debias text, as it has been shown to be effictive in 
more open-ended situations where the chain of thought can't be accurately modelled 
for general cases \parencite{zhou_etal_2025}. I will also be using RAG, as it has been
shown to provide a good way to provide context, and a factual foundation
\parencite{agada_etal_2025}.

\chapter{Prototype}

\section{Overview}

The prototype aims to explore traditional methodologies in prompt engineering
\parencite{Chen2025PromptEngineeringReview}, and where they might fall short. It
also serves as a guide to indicate how the final system architecture should 
be designed to create the most efficient, and relevant solution.

\section{Bias Neutralisation Subsystem}

\subsection{Identifying Bias}

\subsubsection{Rationale}

First we need to identify whether or not the original text has any bias in the
first place. There's no point trying to remove the bias in a text that has none.
This step will save costs, and improve the efficiency of the overall process.
\\

For the prototype I decided to use few-shot prompting, in combination with role prompting 
\parencite{Chen2025PromptEngineeringReview}, and system prompting, 
which is quick and easy,
but shown to be unreliable for NLP type tasks, especially when asked to explain
their chain of reasoning \parencite{ye_durrett_2022}.
Therefore I will only be using it as a placeholder for other methods I plan to
implement, namely RAG.

\subsubsection{Algorithm}

\begin{algorithm}[H]
  \caption{Bias Tagger}
  \begin{algorithmic}
    \Require userInput : string
    \Require exampleData : Array[(string, string)]
    \Require LLM : string $\to$ string

    \\

    \State $\text{Role} : \text{enum} \gets \{\text{SYSTEM}, \text{USER}, \text{ASSISTANT}\}$
    \State messages : Array[(Role, string)]
    \\

    \State messages $\gets$ append $\left(
      \text{Role.SYSTEM},
      \text{'tag biased sections'}
    \right)$

    \\


    \For{$(\text{original, tagged})$ in $\text{exampleData}$}
      \State messages $\gets$ append{(Role.USER, original)}
      \State messages $\gets$ append{(Role.ASSISTANT, tagged)}
    \EndFor

    \\

    \State LLM.messages $\gets$ messages
    \\
    \State taggedText $\gets$ LLM(userInput)

    \\
    \\

    \Return taggedText

  \end{algorithmic}
\end{algorithm}


\subsection{Parsing and Removing Bias}

\subsubsection{Rationale}

For the parsing I decided to use Regex, which is a standard way to parse strings
I scan the tagged string to see if it actually has any tags in it. I then append
all of the tagged sections into an array. My reasoning for this is that by creating
an array of biased sections to rewrite, rather than rewriting the whole document, I
can take advantage of two things: 

\begin{enumerate}
  \item Parallelisation
  \item Enhanced user interactivity
\end{enumerate}

Point 1 is key for performance, I have essentially created a job queue, where each
job is independent of the other. This is the perfect opportunity to parallelise the
workload. This will speed up the performance of the system, where the LLMs only
have to rewrite a small amount of text at each time.
\\
\\
Point 2 is more related to user interaction, I plan to allow the user to select which
parts of the text to rewrite, giving them control over what is re-written in their text.
It will also be useful for highlighting which parts of the text are biased, and also add
additional annotations to these parts of the text.
\\

Overall this method will reduce costs, because even if the user decides to rewrite all
of the biased sections, it will only be a subset of the whole text.
\\

For the actual de-biasing of the sections, I'm using an LLM post-trained through
few-shot prompting.

\subsubsection{Algorithm}

\begin{algorithm}[H]
\caption{Text Parser}
\begin{algorithmic}[1]
\Require $taggedText$ : string
\Ensure $biasedSections$ : array of strings

\State $regexPattern \gets \langle\!\langle (.*?) \rangle\!\rangle$
\State $biasedSections \gets$ empty array

\State $matches \gets \text{matchAll}(taggedText,\ regexPattern)$
\ForAll{$match \in matches$}
    \State Append $match$ to $biasedSections$
\EndFor

\State \Return $biasedSections$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Neutralise Bias}
\begin{algorithmic}[1]
\Require $section$ : string
\Require $LLM$ : function $(\text{string}) \rightarrow (\text{string})$
\Require $exampleData$ : array of $(string,\ string)$ pairs
\Ensure $neutralisedSection$ : string

\State Define enum $Role \gets \{\text{SYSTEM},\ \text{USER},\ \text{ASSISTANT}\}$
\State $messages \gets$ empty array of $(Role,\ string)$

\State Append $(Role.SYSTEM,\ \text{"neutralise the bias in the text"})$ to $messages$

\ForAll{$(original,\ neutralised) \in exampleData$}
    \State Append $(Role.USER,\ original)$ to $messages$
    \State Append $(Role.ASSISTANT,\ neutralised)$ to $messages$
\EndFor

\State $LLM.messages \gets messages$
\State $neutralisedSection \gets LLM(section)$

\State \Return $neutralisedSection$
\end{algorithmic}
\end{algorithm}

\subsection{Creating a RESTful API}

\subsubsection{Rationale}

I decided to use REST APIs to encapsulate all of the functionality, and allow
a front-end to call the API. This will provide the user with a GUI, and allow 
them to interact with the system. For the
prototype I implemented a web based GUI, using Typescript and React.

The prototype has a single endpoint:

\begin{verbatim}
      (POST, "/remove-bias")
\end{verbatim}

which removes the bias of the whole text, in the post body:

\begin{verbatim}
  {text: string}
\end{verbatim}

The above outlines a valid json request to the endpoint.

\subsubsection{Algorithm}

\begin{algorithm}[H]
\caption{Bias Removal API}
\begin{algorithmic}[1]
\Require Route $(\text{POST},\ \texttt{"/remove-bias"})$
\Require $req$ : HTTP request
\Ensure JSON response

\State $body \gets req.\text{getBody()}$
\If{$body$ is invalid}
    \State \Return JSON$\big(\{\text{"error"}:\ \text{"invalid request"}\}\big)$ with status $400$
\EndIf

\State $text \gets body[\text{"text"}]$
\If{$text$ is empty}
    \State \Return JSON$\big(\{\text{"error"}:\ \text{"invalid request"}\}\big)$ with status $400$
\EndIf

\State $taggedText \gets \text{tagText}(text)$
\State $taggedSections \gets \text{getTaggedSections}(taggedText)$

\If{$taggedSections$ is empty}
    \State \Return JSON$\big(\{\text{"result"}:\ text\}\big)$ with status $200$
\EndIf

\State $neutralisedSections \gets$ empty array

\ForAll{$section \in taggedSections$}
    \State Append $\text{neutraliseSection}(section)$ to $neutralisedSections$
\EndFor

\State $neutralisedText \gets \text{replaceTaggedSections}(taggedText,\ neutralisedSections)$

\State \Return JSON$\big(\{\text{"result"}:\ neutralisedText\}\big)$ with status $200$
\end{algorithmic}
\end{algorithm}

\subsubsection{Front-End}

\includegraphics[width=\textwidth]{images/front-end-prototype.jpg}

The front-end is extremely bare-bones, literally just an input text box, a submit
button which sends the text to the api, and an output box to display the result.

\subsection{Evalutation of Initial Prototype}

The initial prototype works, but barely. It can turn an explicity biased text into
one that is implicity biased. Wich might be worse in a way, since implicit bias
is far more problematic.

For example take the following test case - biased text generated 
by chatGPT \parencite{chatgpt2025}:

\begin{verbatim}
tests/test_tag_biased_sections The manager claimed that 
<>the younger women on the team shouldn’t handle negotiations because 
they’re too emotional<>, and instead suggested giving the important 
tasks to <>the older men, who he said are naturally more reliable<>. 
He also remarked that <>employees from that specific Asian community usually 
struggle with leadership roles and should stick to basic support work<>.
\end{verbatim}


\begin{verbatim}
tests/test_rest_api.py::test_remove_bias The manager claimed that 
Team members assigned to negotiations should be based on skills and 
experience rather than assumptions about emotions, and instead suggested 
giving the important tasks to the older men, who he said Some individuals 
tend to be more reliable. He also remarked that Some employees may find 
leadership roles more challenging and may excel in support work, 
regardless of their community background.
\end{verbatim}

It's very obvious about what the bias here is, even if it isn't explicitly stated, for example
take the section "skills and experience rather than assumptions about emotions, and instead 
suggested giving the important tasks to the older men", It's obvious that this is targeted
at young women, or young people in general even if the bias isn't explicity stated.

Another point of failure is that the sentence might sometimes just stop making sense: "to 
the older men, who he said Some individuals tend to be more reliable," this sentence
clearly has some grammatical issues.
\\

These are the main limitations of the prototype as it is.

\section{Prototype Improvements}

\subsection{Overview}

To enhance the prototype, I decided to add the following features to the prototype
to improve the system architecture, as well as the overall performance.

\begin{itemize}
  \item Improved prompt-engineering for more descriptive tagging
  \item Improved context, to improve how well the rewrite fits with the text
    by providing the LLM with additional context.
  \item Highlighting subsections of the text, and allowing for specific sub-
    sections to be rewritten.
  \item  Containerising the front-end, and back-end.
\end{itemize}

\subsection{Improved Tagging}

\subsubsection{Rationale}

I wanted to implement improved tagging so that I had more control over
the outcome of the LLM. I wanted to distinguish between different sources of
bias, which requires more descriptive tags.

\subsubsection{Implementation}

To improve the tagging, I had to improve the prompts used currently. For this I
used a technique inspired by a concept called meta-prompting \parencite{ye_axmed_pryzant_khani_2024},
which simply involves using an LLM to iteratively create better prompts.

So, I used ChatGPT for the meta-prompting, and passed in the previous prompts, whilst
specifying that I wanted tags that could would determine a bias type.

\begin{verbatim}
"content": (
"You are a bias detection system. "
"Identify explicitly biased language in text. "
"Surround ONLY the biased phrase(s) with XML-style tags indicating the bias type. "
"Use ONLY the following tags: <gender>, <age>, <race>. "
"If multiple biased phrases exist, tag ALL of them. "
"Do NOT rephrase the sentence. "
"Do NOT add explanations. "
"If no bias exists, return the original text unchanged. "
"Tag only the biased claim itself, not consequences or actions. "
"If a biased phrase could belong to multiple categories, choose the single most appropriate one."
)
\end{verbatim}
\parencite{chatgpt2025}

After iterating with the meta-prompting methodology \parencite{ye_axmed_pryzant_khani_2024},
this is the output of the LLM.

I also improved the general performance by including more diverse examples for the
few-shot prompting methodology \parencite{Chen2025PromptEngineeringReview}:

\begin{verbatim}
{
"role": "user",
"content": (
"During the hiring discussion, the director said that women are too emotional "
"for leadership roles, that older employees struggle to adapt to new technologies, "
"and that certain racial groups are naturally less intelligent."
)
},
{
"role": "assistant",
"content": (
"During the hiring discussion, the director said that "
"<gender>women are too emotional for leadership roles</gender>, "
"that <age>older employees struggle to adapt to new technologies</age>, "
"and that <race>certain racial groups are naturally less intelligent</race>."
)
},
\end{verbatim}

This is an example where a sentence has multiple sources of bias.

\begin{verbatim}
{
"role": "user",
"content": (
"The team decided to reschedule the meeting due to conflicting availability."
)
},
{
"role": "assistant",
"content": (
"The team decided to reschedule the meeting due to conflicting availability."
)
},
\end{verbatim}

This is an example where there are no sources of bias. I did this so that the 
LLM could better handle any edge cases.

\subsection{Improved Context}

\subsubsection{Rationale}

One of the main problems of the initial prototype was that it sometimes produced
results that were grammatically incorrect. To fix this I decided to implement
contextual prompting \parencite{Chen2025PromptEngineeringReview}. But, to do this I had to dynamically
generate the context for each biased section.

I did this mainly through the use of regular expressions, and also an added ID field
to the tags to identify which context belonged to what tag.

\subsubsection{Algorithm}



\begin{algorithm}[H]
\caption{Split Context into Normal and Biased Segments}
\begin{algorithmic}[1]
\Require $text$ : input string
\Ensure $segments$ : list of segmented text blocks

\State $sentences \gets$ split $text$ using regex \texttt{(?<=[.!?])\textbackslash s+}
\State $segments \gets$ empty list

\ForAll{$sentence \in sentences$}
    \State $cursor \gets 0$

    \ForAll{$match \in$ \texttt{BIAS\_PATTERN.finditer(sentence)}}
        \State $(start, end) \gets$ span of $match$

        \If{$start > cursor$}
            \State $normal\_text \gets$ trim($sentence[cursor : start]$)
            \If{$normal\_text \neq \emptyset$}
                \State Append $\{type:\text{"normal"},\ text: normal\_text\}$ to $segments$
            \EndIf
        \EndIf

        \State Append $\{type:\text{"bias"},\ bias\_type: match.group(1),\ text: \text{trim}(match.group(2))\}$ to $segments$
        \State $cursor \gets end$
    \EndFor

    \If{$cursor < \text{length}(sentence)$}
        \State $normal\_text \gets$ trim($sentence[cursor : ]$)
        \If{$normal\_text \neq \emptyset$}
            \State Append $\{type:\text{"normal"},\ text: normal\_text\}$ to $segments$
        \EndIf
    \EndIf
\EndFor

\State \Return $segments$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Parse Biased Sections with Context Window}
\begin{algorithmic}[1]
\Require $text$ : string
\Require $context\_window$ : integer
\Ensure $results$ : array of records

\State $segments \gets \text{splitContext}(text)$
\State $results \gets$ empty array
\State $section\_id \gets 0$

\For{$i \gets 0$ to $\text{length}(segments) - 1$}
    \If{$segments[i].type \neq \text{"bias"}$}
        \State \textbf{continue}
    \EndIf

    \State $ctx\_start \gets \max(0,\ i - context\_window)$
    \State $ctx\_end \gets \min(\text{length}(segments) - 1,\ i + context\_window)$

    \State $context\_parts \gets$ empty list
    \For{$j \gets ctx\_start$ to $ctx\_end - 1$}
        \State Append $segments[j].text$ to $context\_parts$
    \EndFor

    \State Append $\{$
        $context:\ \text{join}(context\_parts,\ " "),$
        $section\_id:\ section\_id,$
        $text:\ segments[i].text$
    $\}$ to $results$

    \State $section\_id \gets section\_id + 1$
\EndFor

\State \Return $results$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{Adding IDs to Opening Tags}
\begin{algorithmic}[1]
\Require $tagged\_text$ : string containing tagged elements
\Ensure $tagged\_text$ with unique IDs added to opening tags

\State $id \gets 0$
\State $i \gets 0$

\While{$i < \text{length}(tagged\_text)$}
    \If{$tagged\_text[i] = \langle$ \textbf{and} $tagged\_text[i+1] \neq /$}
        \State Insert string \texttt{"id:"}$id$\texttt{" "} at position $i+1$ in $tagged\_text$
        \State $id \gets id + 1$
    \EndIf
    \State $i \gets i + 1$
\EndWhile

\State \Return $tagged\_text$
\end{algorithmic}
\end{algorithm}

\subsubsection{Algorithms Explanation}

The second algorithm splits the text by biased sections and sentences so if a sentence
has a biased section: e.g. 
\verb|This is an example <gender id=1> sentence|
\verb|that </gender> contains bias.|

it would be parsed as:

\begin{verbatim}
[
  {
    type: normal,
    text: This is an example
  },
  {
    type: bias,
    bias_type: gender,
    text: sentence that
  },
  {
    type: normal,
    text: contains bias.
  }
]
\end{verbatim}

The second algorithm produces the context for each bias section, and associates
each section with an ID. If we have a context window $n$, it simply looks before 
and after the biased section by $n$ sections, and joins them to create the context.

For example if we have a context window of $2$, and we have a biased section at
index $5$, the context would be constructed by joining all of the sections between
indexes $3$ to $7$.

The final algorithm simply adds the id to the tag, such that they match the section
ids. This way we can understand which context belongs to which section, and where
to locate the section.

\subsection{Highlighting and Rewriting Subsections}

\subsubsection{Rationale}

Where in the previous version, users had no control over what part was
rewritten, here I aim to give the users full control. Sections identified as
bias will be colour-coded, and highlighted. The user can click on the highlighted
section to rewrite that specific section of the text.

This is to improve user interactivity, and allow users to see where sources
of bias are. It also further optimise costs, since the user doesn't have to
rewrite every section if they don't want to, which minimises the requests sent
to the LLM.

\subsubsection{Demo}

I implemented this in React, so It's quite self-explanatory how everything works
from just looking at how it looks:


\includegraphics[width=\textwidth]{images/front-end-highlighting-demo.jpg}

This is an example after some bias text is inserted, text generated by ChatGPT
\parencite{chatgpt2025}. So age based bias is highlighted with blue, gender bias
is highlighted with yellow, and race based bias is highlighted in pink.


\includegraphics[width=\textwidth]{images/front-end-rewriting-demo.jpg}

Here I have selected specific parts of the text to rewrite.

\subsection{Deployment}

For the deployment I have created Docker images for the front-end, and the
back-end separately. This will make it easier to scale the application for
more intensive tasks, and it will also help in modularising the application further.

\section{Evaluation}

After trying out a few different techniques, it's obvious that relying solely on a
traditional engineering approach isn't good enough. Even after added context, there 
are still issues with grammar such as capitalisation, and the LLM seems to still be 
rewriting the text in such a way that it turns explicit bias into more subtle implicit 
bias.
\\

As stated in the introduction, to make the system more reliable, I plan to implement
RAG, and reinforcement learning.

\clearpage

\printbibliography

\end{document}
